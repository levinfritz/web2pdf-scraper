# ğŸ§¾ M129 PDF-Scraper

Dieses Projekt lÃ¤dt automatisch alle Kapitel einer Webseite, speichert jedes Kapitel als PDF und fÃ¼gt sie anschliessend zu einem einzigen Gesamtdokument zusammen.

## ğŸ“¦ Voraussetzungen

- [Node.js](https://nodejs.org/) (Version â‰¥ 14 empfohlen)
- Internetverbindung (die Seiten werden live geladen)

## ğŸ› ï¸ Installation

1. Projekt klonen und Ordner Ã¶ffnen:

   ```bash
   git clone https://github.com/levinfritz/web2pdf-scraper.git
   cd web2pdf-scraper
   ```

2. Projekt initialisieren (falls nÃ¶tig):

   ```bash
   npm init -y
   ```

3. AbhÃ¤ngigkeiten installieren:

   ```bash
   npm install puppeteer pdf-lib
   ```

4. Datei `scrape_m129.js` erstellen (siehe Beispielcode in diesem Repository)

## â–¶ï¸ AusfÃ¼hrung

Das Skript startest du mit:

```bash
node scrape_m129.js
```

Das Skript fÃ¼hrt folgende Schritte aus:

- LÃ¤dt die Hauptseite
- Extrahiert automatisch alle Kapitel-Links
- Rendert jede Seite mit Puppeteer und speichert sie als `Kapitel_01.pdf`, `Kapitel_02.pdf` usw.
- FÃ¼gt alle PDFs zu einem Dokument `M129_Gesamt.pdf` zusammen
- LÃ¶scht die Einzeldokumente nach der ZusammenfÃ¼hrung automatisch

## ğŸ“ Ergebnis

Nach dem Durchlauf findest du im Ordner `m129_pdfs`:

```text
m129_pdfs/
â””â”€â”€ M129_Gesamt.pdf âœ…
```

## ğŸ§¹ AufrÃ¤umen

Die temporÃ¤ren Kapitel-PDFs werden nach der ZusammenfÃ¼hrung automatisch gelÃ¶scht. Nur das finale Dokument bleibt erhalten.

## ğŸ“„ Lizenz

MIT-Lizenz â€“ frei verwendbar, auch fÃ¼r den Schulgebrauch.
