# ğŸ§¾ M129 PDF-Scraper

Dieses Projekt lÃ¤dt automatisch alle Kapitel der Webseite, speichert jedes Kapitel als PDF und fÃ¼gt sie anschliessend zu einem einzigen Gesamtdokument zusammen.

## ğŸ“¦ Voraussetzungen

- [Node.js](https://nodejs.org/) (Version â‰¥ 14 empfohlen)
- Internetverbindung (die Seiten werden live geladen)

## ğŸ› ï¸ Installation

1. Projektordner erstellen und hineingehen:

   ```bash
   git clone https://github.com/levinfritz/web2pdf-scraper.git
   cd web2pdf-scraper
````

2. Projekt initialisieren:

   ```bash
   npm init -y
   ```

3. BenÃ¶tigte AbhÃ¤ngigkeiten installieren:

   ```bash
   npm install puppeteer pdf-lib
   ```

4. Datei `scrape_m129.js` erstellen (Code siehe `scrape_m129.js` in diesem Projekt)

## â–¶ï¸ AusfÃ¼hrung

Starte das Script mit folgendem Befehl:

```bash
node scrape_m129.js
```

Das Script macht folgendes:

* LÃ¤dt die Hauptseite 
* Extrahiert automatisch alle Kapitel-Links
* Rendert jede Seite mit Puppeteer und speichert sie als `Kapitel_01.pdf`, `Kapitel_02.pdf`, â€¦
* FÃ¼hrt alle PDFs zu einem Dokument `M129_Gesamt.pdf` zusammen
* LÃ¶scht die Einzeldokumente danach automatisch

## ğŸ“ Ergebnis

Nach dem Durchlauf findest du im Ordner `m129_pdfs`:

```text
m129_pdfs/
â””â”€â”€ M129_Gesamt.pdf âœ…
```

## ğŸ§¹ Cleanup

Die temporÃ¤ren Kapitel-PDFs werden automatisch gelÃ¶scht. Nur das finale Dokument bleibt erhalten.

## ğŸ“„ Lizenz

MIT-Lizenz â€“ frei verwendbar, auch fÃ¼r den Schulgebrauch.

```

---

Wenn du willst, kann ich dir auch die passende `.zip`-Struktur generieren oder eine GitHub-kompatible Vorlage mit `package.json`, `.gitignore` etc. vorbereiten. Sag einfach Bescheid!
```
